{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d40c325",
   "metadata": {},
   "source": [
    "# üß† PIL Language Model Training\n",
    "\n",
    "**Hybrid Transformer with Pseudoinverse Learning (PIL)**\n",
    "\n",
    "This notebook trains a language model that uses PIL (gradient-free) for FFN layers instead of backpropagation.\n",
    "\n",
    "---\n",
    "**Quick Start:**\n",
    "1. Runtime ‚Üí Change runtime type ‚Üí GPU (T4)\n",
    "2. Run all cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04868339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Clone the repository\n",
    "!git clone https://github.com/sanjuz-cas/PIL.git\n",
    "%cd PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eede386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Install dependencies\n",
    "!pip install -q torch datasets transformers tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaf6e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Verify installation\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d5bb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Run training on WikiText-2\n",
    "!python examples/train_pil_lm.py \\\n",
    "    --dataset wikitext \\\n",
    "    --embed_dim 256 \\\n",
    "    --num_layers 4 \\\n",
    "    --max_train_samples 5000 \\\n",
    "    --max_eval_samples 1000 \\\n",
    "    --num_epochs 1 \\\n",
    "    --device cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca55379",
   "metadata": {},
   "source": [
    "## üî¨ Interactive Testing\n",
    "\n",
    "Run the cells below to test the model interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39d5b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model and test generation\n",
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "from app.core.pil_lm import PILLMConfig, PILLanguageModel\n",
    "from transformers import GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Create model\n",
    "config = PILLMConfig(\n",
    "    vocab_size=50257,\n",
    "    embed_dim=256,\n",
    "    num_heads=4,\n",
    "    num_layers=4,\n",
    "    max_seq_len=128\n",
    ")\n",
    "model = PILLanguageModel(config)\n",
    "\n",
    "# Check if saved model exists\n",
    "import os\n",
    "if os.path.exists('pil_lm_model.pt'):\n",
    "    model.load_state_dict(torch.load('pil_lm_model.pt'))\n",
    "    print(\"Loaded trained model!\")\n",
    "else:\n",
    "    print(\"No saved model found. Using untrained model.\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "print(f\"Model on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7314042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "def generate_text(prompt, max_tokens=50, temperature=0.8):\n",
    "    tokens = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            tokens,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_k=50,\n",
    "            top_p=0.9\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Test prompts\n",
    "prompts = [\n",
    "    \"The future of artificial intelligence\",\n",
    "    \"Once upon a time\",\n",
    "    \"In the year 2050\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nüìù Prompt: {prompt}\")\n",
    "    print(f\"ü§ñ Generated: {generate_text(prompt)}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f81e37",
   "metadata": {},
   "source": [
    "## üìä Training with Custom Parameters\n",
    "\n",
    "Adjust these parameters based on your needs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2084ed98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom training with larger model (use GPU!)\n",
    "!python examples/train_pil_lm.py \\\n",
    "    --dataset wikitext \\\n",
    "    --embed_dim 384 \\\n",
    "    --num_layers 6 \\\n",
    "    --num_heads 6 \\\n",
    "    --max_train_samples 10000 \\\n",
    "    --max_eval_samples 2000 \\\n",
    "    --num_epochs 3 \\\n",
    "    --device cuda"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
